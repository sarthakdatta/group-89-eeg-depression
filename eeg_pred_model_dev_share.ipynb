{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takeisika/group-89-eeg-depression/blob/main/eeg_pred_model_dev_share.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "464SROaR_HmJ",
        "outputId": "9b82b01a-b6d6-4584-d28f-509d34c9c623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-LrdJbU_Ps4",
        "outputId": "57d0dbc4-5ec7-4348-a332-6c604d6e8b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/EEG_128channels_resting_lanzhou_2015.zip\n",
            "   creating: /content/data/EEG_128channels_resting_lanzhou_2015/\n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010013rest 20150703 1333..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010012rest 20150626 1026..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020022rest 20150707 1452..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/Multivariate Pattern Analysis of EEG-Based Functional Connectivity A Study on the Identification of Depression.pdf  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010019rest 20150716 1440..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030020_rest 20151230 1416.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010005rest 20150507 0907..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010022restnew 20150724 14.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010033rest 20160331 1239..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010015rest 20150709 1456..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020029rest 20150715 1316..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020026_rest 20150714 1413.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010026rest 20160311 1421..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010006rest 20150528 0928..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030014rest 20151117 1441..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030021rest 20160105 1141..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030004_rest 20151026 1930.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010036_rest 20160408 1418.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020023restnew 20150709 10.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010025 20160311 1206.mat.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020018rest 20150702 1651..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020027rest 20150713 1049..mat  "
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/EEG_128channels_resting_lanzhou_2015.zip -d /content/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZaXf9OLJeiK"
      },
      "source": [
        "# ❶ Get all .mat files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biM6MrYiJgwn",
        "outputId": "04269882-8060-4655-adc7-325610b11ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 53 .mat files.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "FOLDER = \"/content/data\"\n",
        "mat_files = glob.glob(os.path.join(FOLDER, \"**/*.mat\"), recursive=True)\n",
        "print(f\"Found {len(mat_files)} .mat files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7C4r2DVVk_G"
      },
      "source": [
        "# ❷ Get subj_id & its label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZF1-5V4JjuO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "def get_subj_id_and_label(path):\n",
        "    filename = os.path.basename(path)\n",
        "    match = re.search(r\"(02\\d+)\", filename)  # 02010002_... or 02020008_... or 02030002_...\n",
        "    subj_id = match.group(1)\n",
        "    label = 1 if subj_id.startswith(\"0201\") else 0  # 0201...→depressed (1), others→not depressed(0)\n",
        "    return subj_id, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5cf5lO0K0g7"
      },
      "source": [
        "# ❸ Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS9P_V3jK2-o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import signal\n",
        "import random\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "SAMPLING_RATE = 250\n",
        "LOW_CUTOFF = 1.0\n",
        "HIGH_CUTOFF = 45.0\n",
        "NOTCH = 50.0\n",
        "TRIM_SEC = 30\n",
        "\n",
        "WIN_SEC = 2.0\n",
        "STEP_SEC = WIN_SEC / 2\n",
        "\n",
        "def butter_bandpass_filter(data, sampling_rate, low_cutoff, high_cutoff, filter_order=4):\n",
        "    nyquist_freq = 0.5 * sampling_rate\n",
        "    low_norm = low_cutoff / nyquist_freq\n",
        "    high_norm = high_cutoff / nyquist_freq\n",
        "    b_coeffs, a_coeffs = signal.butter(filter_order, [low_norm, high_norm], btype='band')\n",
        "    filtered_data = signal.filtfilt(b_coeffs, a_coeffs, data, axis=-1)\n",
        "    return filtered_data\n",
        "\n",
        "def notch_filter(data, sampling_rate, notch, quality_factor=30.0):\n",
        "    nyquist_freq = 0.5 * sampling_rate\n",
        "    notch_norm = notch / nyquist_freq\n",
        "    b_coeffs, a_coeffs = signal.iirnotch(w0=notch_norm, Q=quality_factor)\n",
        "    filtered_data = signal.filtfilt(b_coeffs, a_coeffs, data, axis=-1)\n",
        "    return filtered_data\n",
        "\n",
        "def preprocess_data(data, sampling_rate, low_cutoff=LOW_CUTOFF, high_cutoff=HIGH_CUTOFF, notch=NOTCH, trim_sec=TRIM_SEC):\n",
        "    processed_data = data - data.mean(axis=0, keepdims=True)\n",
        "    processed_data = butter_bandpass_filter(processed_data, sampling_rate, low_cutoff, high_cutoff)\n",
        "    processed_data = notch_filter(processed_data, sampling_rate, notch)\n",
        "    start_sample = int(trim_sec * sampling_rate)\n",
        "    end_sample = processed_data.shape[1] - int(trim_sec * sampling_rate)\n",
        "    end_sample = max(end_sample, start_sample + 1)\n",
        "    return processed_data[:, start_sample:end_sample]\n",
        "\n",
        "def slide_wins(preprocessed_data, sampling_rate, win_sec=WIN_SEC, step_sec=STEP_SEC):\n",
        "    win_cnts = int(win_sec * sampling_rate)\n",
        "    step_cnts = int(step_sec * sampling_rate)\n",
        "\n",
        "    wins = []\n",
        "    win_idxs = []\n",
        "    for start_idx in range(0, preprocessed_data.shape[1] - win_cnts + 1, step_cnts):\n",
        "        end_idx = start_idx + win_cnts\n",
        "        wins.append(preprocessed_data[:, start_idx:end_idx])\n",
        "        win_idxs.append((start_idx, end_idx))\n",
        "\n",
        "    if wins:\n",
        "        wins_array = np.stack(wins, axis=0)\n",
        "    else:\n",
        "        wins_array = np.empty((0, preprocessed_data.shape[0], win_cnts))\n",
        "\n",
        "    return wins_array, win_idxs\n",
        "\n",
        "def discard_noisy_wins(wins_array, z_score_threshold=7.0):\n",
        "    if len(wins_array) == 0:\n",
        "        return wins_array\n",
        "    mean = wins_array.mean(axis=(1, 2), keepdims=True)\n",
        "    std = wins_array.std(axis=(1, 2), keepdims=True) + 1e-6\n",
        "    z_scores = (wins_array - mean) / std\n",
        "    is_not_noisy_win_tf = np.max(np.abs(z_scores), axis=(1, 2)) < z_score_threshold\n",
        "    return wins_array[is_not_noisy_win_tf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa98ZdUNK71d"
      },
      "source": [
        "# ❹ Feature Extraction (Welch's Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5NcBY7JK9xH"
      },
      "outputs": [],
      "source": [
        "FREQ_BANDS = [\n",
        "    (1, 4),    # Delta (δ)\n",
        "    (4, 8),    # Theta (θ)\n",
        "    (8, 13),   # Alpha (α)\n",
        "    (13, 30),  # Beta (β)\n",
        "    (30, 45)   # Gamma (γ)\n",
        "]\n",
        "\n",
        "def welch(wins_array, sampling_rate=SAMPLING_RATE, freq_bands=FREQ_BANDS):\n",
        "    if len(wins_array) == 0:\n",
        "        return np.empty((0, 0), dtype=np.float32)\n",
        "\n",
        "    win_cnts, ch_cnts, sample_cnts = wins_array.shape\n",
        "    samples_per_seg = 256\n",
        "    features = []\n",
        "\n",
        "    for idx in range(win_cnts):\n",
        "        curr_win = wins_array[idx]\n",
        "        freqs, psds = signal.welch(curr_win, fs=sampling_rate, nperseg=samples_per_seg, axis=-1)\n",
        "        integrated_psd_in_all_freqs = np.trapz(psds, freqs, axis=-1) + 1e-12\n",
        "\n",
        "        curr_win_bands = []\n",
        "        for (low_freq, high_freq) in freq_bands:\n",
        "            is_in_this_freq_band_tf = (freqs >= low_freq) & (freqs < high_freq)\n",
        "            integrated_psd_in_this_freq_band = np.trapz(psds[:, is_in_this_freq_band_tf], freqs[is_in_this_freq_band_tf], axis=-1)\n",
        "            relative_psd_in_this_freq_band_in_perc = integrated_psd_in_this_freq_band / integrated_psd_in_all_freqs\n",
        "            curr_win_bands.append(relative_psd_in_this_freq_band_in_perc)\n",
        "\n",
        "        win_features = np.stack(curr_win_bands, axis=-1)\n",
        "        features.append(win_features)\n",
        "\n",
        "    features_array = np.stack(features, axis=0)\n",
        "    features_array = features_array.reshape(win_cnts, -1).astype(np.float32)\n",
        "\n",
        "    return features_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2S9MV48OeCY"
      },
      "source": [
        "# ❺ Load Target Data from .mat Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqbe0dmqOgaE"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "\n",
        "def load_mat(path, verbose=True):\n",
        "    mat = loadmat(path, squeeze_me=True, struct_as_record=False)\n",
        "    keys_wo__ = []\n",
        "    for keyname in mat.keys():\n",
        "        if not keyname.startswith(\"__\"):\n",
        "            keys_wo__.append(keyname)\n",
        "    target_key = keys_wo__[0]\n",
        "    target_data = np.asarray(mat[target_key], dtype=np.float32)\n",
        "\n",
        "    if target_data.shape[0] == 129:\n",
        "        target_data = target_data[:128, :]\n",
        "\n",
        "    return target_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ❻ Build Dataset for ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q uninstall -y tensorflow-decision-forests tensorflow-text tf-keras > /dev/null 2>&1 || true\n",
        "!pip -q install \"tensorflow==2.18.1\" numpy scipy scikit-learn h5py > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dataset(files):\n",
        "    X_all, y_all, subj_all = [], [], []\n",
        "    skipped = []\n",
        "    for file in sorted(files):\n",
        "        subj_id, y = get_subj_id_and_label(file)\n",
        "        data = load_mat(file)\n",
        "        processed_data = preprocess_data(data, sampling_rate=SAMPLING_RATE)\n",
        "        wins_array, _ = slide_wins(processed_data, sampling_rate=SAMPLING_RATE)\n",
        "        wins_array = discard_noisy_wins(wins_array, z_score_threshold=7.0)\n",
        "        if len(wins_array) == 0:\n",
        "            skipped.append((file, \"no_windows_after_noise_removal\"))\n",
        "            continue\n",
        "        features_array = welch(wins_array)\n",
        "        X_all.append(features_array)\n",
        "        y_all.append(np.full((features_array.shape[0],), y, dtype=np.int64)) # For 500 windows: [1, 1, 1, ..., 1, 1] (array of length 500)\n",
        "        subj_all.append(np.full((features_array.shape[0],), subj_id, dtype=object)) # For 500 windows: [\"02010002\", \"02010002\", ..., \"02010002\"] (array of length 500)\n",
        "    return np.concatenate(X_all, 0), np.concatenate(y_all, 0), np.concatenate(subj_all, 0), skipped\n",
        "\n",
        "X_all, y_all, subj_all, skipped = build_dataset(mat_files)\n",
        "print(\"Windows:\", X_all.shape, \"Proportion of depression data:\", y_all.mean(), \" Unique subjects:\", len(set(subj_all)))\n",
        "print(f\"Skipped files: {len(skipped)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ❼ Split Data by Subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "trval_test = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
        "trval_idxs, test_idxs = next(trval_test.split(X_all, y_all, groups=subj_all))\n",
        "X_trval, y_trval, subj_trval = X_all[trval_idxs], y_all[trval_idxs], subj_all[trval_idxs]\n",
        "X_test, y_test, subj_test = X_all[test_idxs], y_all[test_idxs], subj_all[test_idxs]\n",
        "\n",
        "tr_val = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
        "tr_idxs, val_idxs = next(tr_val.split(X_trval, y_trval, groups=subj_trval))\n",
        "\n",
        "X_tr, y_tr = X_trval[tr_idxs], y_trval[tr_idxs]\n",
        "X_val, y_val = X_trval[val_idxs], y_trval[val_idxs]\n",
        "\n",
        "print(f\"Train: {X_tr.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ❽ Standardize Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_tr  = scaler.fit_transform(X_tr)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test= scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ❾ Build MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "def build_mlp(input_dim):\n",
        "    input_layer = tf.keras.Input(shape=(input_dim,), name=\"input_features\")\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(input_layer)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(input_layer, output_layer)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "model = build_mlp(X_tr.shape[1])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=8, restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', mode='max', patience=4, factor=0.5, min_lr=1e-5)]\n",
        "\n",
        "class_weight = {0: 1.0, 1: 3.0}\n",
        "\n",
        "model.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=60, batch_size=256, callbacks=callbacks, class_weight=class_weight, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ➓ Subject-level Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def acc_by_subj(model, Xs, ys, subj_ids):\n",
        "    win_probs = model.predict(Xs, batch_size=1024, verbose=0).ravel()\n",
        "    win_probs_by_subj_dict = {}\n",
        "    label_by_subj_dict = {}\n",
        "\n",
        "    for win_prob, y, subj_id in zip(win_probs, ys, subj_ids):\n",
        "        win_probs_by_subj_dict.setdefault(subj_id, []).append(win_prob)\n",
        "        label_by_subj_dict[subj_id] = y\n",
        "\n",
        "    preds_by_subj = []\n",
        "    label_by_subj = []\n",
        "\n",
        "    for subj_id in sorted(win_probs_by_subj_dict.keys()):\n",
        "        label_by_subj.append(label_by_subj_dict[subj_id])\n",
        "        avg_prob_by_subj = np.mean(win_probs_by_subj_dict[subj_id])\n",
        "        pred_by_subj = 1 if avg_prob_by_subj >= 0.5 else 0\n",
        "        preds_by_subj.append(pred_by_subj)\n",
        "\n",
        "    return (np.array(label_by_subj) == np.array(preds_by_subj)).mean()\n",
        "\n",
        "\n",
        "val_acc  = acc_by_subj(model, X_val, y_val, subj_trval[val_idxs])\n",
        "test_acc = acc_by_subj(model, X_test, y_test, subj_test)\n",
        "print(f\"Subject-level Val Acc: {val_acc:.3f} | Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO_jis_vdElq"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EgL9CLbdGlh"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "model_save_path = '/content/eeg_depression_model.pkl'\n",
        "joblib.dump(model, model_save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPGKymuh+oMOy+ZJeZjwJcX",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
